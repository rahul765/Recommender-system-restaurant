{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here\n",
    "# from sklearn import linear_model\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"RestoInfo.csv\")\n",
    "\n",
    "\n",
    "# Renaming identifier to be used at later stage\n",
    "dataset = dataset.rename(columns={\"Unnamed: 0\":\"user_id\"})\n",
    "\n",
    "\n",
    "# rename non descriptive columns\n",
    "dataset.rename(columns={'approx_cost(for two people)': 'average_cost', 'listed_in(type)': 'meal_type'}, inplace=True)\n",
    "\n",
    "# checking and removing for duplicate rows\n",
    "dataset.duplicated().sum() # Although in this dataset there is no duplicate row\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "# check for null values\n",
    "((dataset.isnull().sum()/dataset.shape[0])*100).round(2)\n",
    "#As we notice around 54 % of data will be lost if we delete the nan values in \"dish_liked\" column We will keep that column for now\n",
    "\n",
    "# Adjust the columns\n",
    "dataset['name'] = dataset['name'].apply(lambda x:x.title())\n",
    "# Adjusting average_cost\n",
    "dataset['average_cost'] = dataset['average_cost'].astype('str')\n",
    "dataset['average_cost'] = dataset['average_cost'].apply(lambda x: x.replace(',',''))\n",
    "dataset['average_cost'] = dataset['average_cost'].astype(\"float\")\n",
    "dataset['average_cost']= dataset['average_cost'].replace(np.NaN,dataset['average_cost'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['average_cost'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['average_cost'] =dataset['average_cost'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = []\n",
    "for word in stopwords.words('english'):\n",
    "    s = [char for char in word if char not in string.punctuation]\n",
    "    stop.append(''.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "# Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return \" \".join([word for word in nopunc.split() if word.lower() not in stop])\n",
    "dataset['reviews_list'] = dataset['reviews_list'].apply(text_process)\n",
    "dataset['name'] = dataset['name'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['reviews_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['restaurant_id'] = le.fit_transform(dataset['name'])\n",
    "\n",
    "dataset['location'] = le.fit_transform(dataset['location'])\n",
    "dataset['cuisines'] = le.fit_transform(dataset['cuisines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['user_id','restaurant_id','location','average_cost','cuisines','reviews_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_df = dataset[['user_id','reviews_list']]\n",
    "restaurant_id_df = dataset[['restaurant_id','reviews_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_df.groupby('user_id').agg({'reviews_list': ' '.join})\n",
    "restaurant_id_df.groupby('restaurant_id').agg({'reviews_list': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#userid vectorizer\n",
    "userid_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=5000)\n",
    "userid_vectors = userid_vectorizer.fit_transform(user_id_df['reviews_list'])\n",
    "\n",
    "#Restaurant vectorizer\n",
    "resturant_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=5000)\n",
    "rest_vectors = resturant_vectorizer.fit_transform(restaurant_id_df['reviews_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = pd.DataFrame(userid_vectors.toarray(), index=user_id_df.index, columns=userid_vectorizer.get_feature_names())\n",
    "Q = pd.DataFrame(rest_vectors.toarray(), index=restaurant_id_df.index, columns=resturant_vectorizer.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_rating_matrix = pd.pivot_table(dataset,values=['location','average_cost','cuisines'] index=['user_id'], columns=['restaurant_id'])\n",
    "userid_rating_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_rating_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R,P,Q,steps=100,gamma=0.001,lamda=0.02):\n",
    "    for step in range(steps):\n",
    "        for i in R.index:\n",
    "            for j in R.columns:\n",
    "                if R.loc[i,j]>0:\n",
    "                    eij=R.loc[i,j]-np.dot(P.loc[i],Q.loc[j])\n",
    "                    P.loc[i]=P.loc[i]+gamma*(eij*Q.loc[j]-lamda*P.loc[i])\n",
    "                    Q.loc[j]=Q.loc[j]+gamma*(eij*P.loc[i]-lamda*Q.loc[j])\n",
    "        e=0\n",
    "        for i in R.index:\n",
    "            for j in R.columns:\n",
    "                if R.loc[i,j]>0:\n",
    "                    e= e + pow(R.loc[i,j]-np.dot(P.loc[i],Q.loc[j]),2)+lamda*(pow(np.linalg.norm(P.loc[i]),2)+pow(np.linalg.norm(Q.loc[j]),2))\n",
    "        if e<0.001:\n",
    "            break\n",
    "        \n",
    "    return P,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "P, Q = matrix_factorization(userid_rating_matrix, P, Q, steps=100, gamma=0.001,lamda=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'good ambiance restaurants, serving fish'\n",
    "test_df= pd.DataFrame([sentence], columns=['reviews_list'])\n",
    "test_df['reviews_list'] = test_df['reviews_list'].apply(clean_text)\n",
    "test_vectors = userid_vectorizer.transform(test_df['text'])\n",
    "test_v_df = pd.DataFrame(test_vectors.toarray(), index=test_df.index, columns=userid_vectorizer.get_feature_names())\n",
    "predict_item_rating=pd.DataFrame(np.dot(test_v_df.loc[0],Q.T),index=Q.index,columns=['location','average_cost','cuisines'])\n",
    "top_recommendations=pd.DataFrame.sort_values(predict_item_rating,['location','average_cost','cuisines'],ascending=[0])[:3]\n",
    "top_recommendations.to_csv('/code/top3_recommendations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
